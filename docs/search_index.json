[["index.html", "1 Time Series Analysis in R", " 1 Time Series Analysis in R This book offers a number of tutorials for analyzing time series in the R programming language. The content is adapted from the PyRATES Timeseries Practicums in python by Julien Emile-Geay authors: David Edge, Nick McKay, and Julien Emile-Geay (with help from Claude.ai) "],["introduction.html", "2 Introduction", " 2 Introduction Content: Data Wrangling, Trend estimation adapted from the Python tutorial by: Julien Emile-Geay, USC Earth Sciences (link) "],["background.html", "2.1 Background", " 2.1 Background Recall that a timeseries X(t) is an ordered data sequence, either continuous or discrete. On a computer, everything is discretized, so regardless of the original process, what we have is a discrete sequence. In timeseries analysis, it is common to consider a timeseries as the sum of one or more oscillatory, stationary components (or signals), a trend, and noise`. The oscillatory components are often the reason we’re doing the analysis in the first place - it will tell us how big they are and what their frequency is. Whether or not those oscillatory components are present, a trend is often present as well. In today’s world the word “trend” is so overused that it has lost all meaning. Here, it will refer to a slowly-evolving, non-stationary component that dominates the behavior of the timeseries. For instance: a linear increase or decrease; a sinusoidal component so slow that its period is not resolved by the dataset; a nonlinear trend like the exponential increase of the Keeling curve (see below). As for noise, it clearly involves a subjective definition. Under this name we usually subsume any variable component in which we are not interested. Some of this noise may be composed of actual measurement errors (what you would think of as noise, strictly speaking), but some of it could be what another analyst would call signal. If you study climate, daily fluctuations are noise; if you study weather, they are your signal. One commonly says that one analyst’s signal is another analyst’s noise. This noise is often modeled as a Normal random process with zero mean (aka Gaussian white noise). To see these concepts in action, let us consider the iconic Mauna Loa Observatory CO2 measurements. "],["data-wrangling.html", "2.2 Data Wrangling", " 2.2 Data Wrangling 2.2.1 Load packages library(dplyr) library(ggplot2) We can pull the data directly from the Scripps Institute website df1 &lt;- read.csv(&quot;https://scrippsco2.ucsd.edu/assets/data/atmospheric/stations/in_situ_co2/monthly/monthly_in_situ_co2_mlo.csv&quot;) head(df1) ## X........................................................................................... ## 1 Atmospheric CO2 concentrations (ppm) derived from in situ air measurements ## 2 at Mauna Loa, Observatory, Hawaii: Latitude 19.5°N Longitude 155.6°W Elevation 3397m ## 3 Since December 2022 sampling has temporarily been relocated to MaunuaKea, Hawaii ## 4 Latitude 19.8°N Longitude 155.5°W Elevation 4145m ## 5 ## 6 Source: R. F. Keeling, S. J. Walker, S. C. Piper and A. F. Bollenbacher Ooo, that doesn’t look right. This file has a big header, and doesn’t quite follow a normal spreadsheet format for the column headers. Let’s skip the header and column names. We can add them back in after loading. df1 &lt;- read.csv(&quot;https://scrippsco2.ucsd.edu/assets/data/atmospheric/stations/in_situ_co2/monthly/monthly_in_situ_co2_mlo.csv&quot;,skip=64,header = F) head(df1) ## V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 ## 1 1958 1 21200 1958.041 -99.99 -99.99 -99.99 -99.99 -99.99 -99.99 MLO ## 2 1958 2 21231 1958.126 -99.99 -99.99 -99.99 -99.99 -99.99 -99.99 MLO ## 3 1958 3 21259 1958.203 315.71 314.44 316.20 314.91 315.71 314.44 MLO ## 4 1958 4 21290 1958.288 317.45 315.16 317.30 314.99 317.45 315.16 MLO ## 5 1958 5 21320 1958.370 317.51 314.69 317.89 315.07 317.51 314.69 MLO ## 6 1958 6 21351 1958.455 -99.99 -99.99 317.27 315.15 317.27 315.15 MLO Okay, not the data looks right. Lets add the column names. names(df1) &lt;- c(&#39;Yr&#39;,&#39;Mn&#39;,&#39;XLDate&#39;,&#39;Date&#39;,&#39;CO2&#39;,&#39;seasonally adjusted&#39;,&#39;fit&#39;,&#39;seasonally adjusted fit&#39;,&#39;CO2 filled&#39;,&#39;seasonally adjusted filled&#39;,&#39;Sta&#39;) head(df1) ## Yr Mn XLDate Date CO2 seasonally adjusted fit seasonally adjusted fit ## 1 1958 1 21200 1958.041 -99.99 -99.99 -99.99 -99.99 ## 2 1958 2 21231 1958.126 -99.99 -99.99 -99.99 -99.99 ## 3 1958 3 21259 1958.203 315.71 314.44 316.20 314.91 ## 4 1958 4 21290 1958.288 317.45 315.16 317.30 314.99 ## 5 1958 5 21320 1958.370 317.51 314.69 317.89 315.07 ## 6 1958 6 21351 1958.455 -99.99 -99.99 317.27 315.15 ## CO2 filled seasonally adjusted filled Sta ## 1 -99.99 -99.99 MLO ## 2 -99.99 -99.99 MLO ## 3 315.71 314.44 MLO ## 4 317.45 315.16 MLO ## 5 317.51 314.69 MLO ## 6 317.27 315.15 MLO The missing value default here is -99.99, let’s change that to NA, R’s standard. df1 &lt;- df1 %&gt;% mutate(across(everything(), ~replace(., . == -99.99 , NA))) Okay, we’re ready for an initial plot ggplot(data=df1, mapping=aes(x=Date, y=CO2)) + geom_line() + labs(title = &quot;Mauna Loa Dataset (latest)&quot;, y=&quot;CO2 (ppm)&quot;) Great, that looks right. "],["trend-extraction.html", "2.3 Trend Extraction", " 2.3 Trend Extraction 2.3.1 Linear trend model The most obvious thing to do is to see if a linear trend might fit the data. In this case it’s likely to be a poor approximation, as the previous plot should make obvious. However, it will be an instructive benchmark for the finer modeling choices we’ll subsequently make. We’ll use the built-in function lm() What is happening under the hood is a simple linear regression, which you don’t need to know much about at this stage. Specifically, we will use the OLS method. which relates a predictand y to a matrix of predictors X, called the design matrix. First, we’ll omit the rows with NA values df1 &lt;- na.omit(df1) CO2model &lt;- lm(CO2~Date, data = df1) summary(CO2model) ## ## Call: ## lm(formula = CO2 ~ Date, data = df1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.2678 -3.6110 -0.9021 3.1466 12.6589 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.887e+03 1.826e+01 -158.1 &lt;2e-16 *** ## Date 1.630e+00 9.171e-03 177.7 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.88 on 785 degrees of freedom ## Multiple R-squared: 0.9758, Adjusted R-squared: 0.9757 ## F-statistic: 3.159e+04 on 1 and 785 DF, p-value: &lt; 2.2e-16 For more information about what each term means, see this post What information can you glean from this summary? What features would you check for in a visual inspection? Let us look at the plot, shall we? How well does the time variable predict CO2? Next, we can use the predict() method to predict CO2 given the design matrix (1, t) df1 &lt;-df1 %&gt;% mutate(linear_model = CO2model$fitted.values) ggplot(data=df1, mapping=aes(x=Date, y=CO2)) + geom_line() + geom_line(data=df1, mapping=aes(x=Date, y=linear_model), inherit.aes = F, color=&quot;red&quot;) + labs(title = &quot;Mauna Loa Dataset (linear model)&quot;, y=&quot;CO2 (ppm)&quot;) We see that the line captures the first order behavior of the series, but that leaves a lot to be desired. To zoom in, we can look at the residuals (original values - fitted values) as a function of fitted values: df1 &lt;-df1 %&gt;% mutate(linear_model_resids = CO2 - CO2model$fitted.values) ggplot(data=df1, mapping=aes(x=Date, y=linear_model_resids)) + geom_line() + labs(title = &quot;Linear Model Residuals Fit&quot;, y=&quot;Residuals&quot;) To be quantitative, we can look at the Root Mean Squared Error of these residuals: sqrt(mean((df1$CO2 - CO2model$fitted.values)^2)) ## [1] 4.874121 This means that this linear trend is, on average, about 5ppm away from the measurements. Can we do better? 2.3.2 Quadratic fit The second thing one might do is to add a quadratic term. CO2_quadratic_model &lt;- lm(CO2 ~ Date + I(Date^2), data = df1) summary(CO2_quadratic_model) ## ## Call: ## lm(formula = CO2 ~ Date + I(Date^2), data = df1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.929 -1.781 0.099 1.814 5.013 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.035e+04 9.756e+02 51.61 &lt;2e-16 *** ## Date -5.184e+01 9.800e-01 -52.90 &lt;2e-16 *** ## I(Date^2) 1.343e-02 2.461e-04 54.57 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.229 on 784 degrees of freedom ## Multiple R-squared: 0.9949, Adjusted R-squared: 0.9949 ## F-statistic: 7.718e+04 on 2 and 784 DF, p-value: &lt; 2.2e-16 df1 &lt;-df1 %&gt;% mutate(quadratic_model = CO2_quadratic_model$fitted.values) ggplot(data=df1, mapping=aes(x=Date, y=CO2)) + geom_line() + geom_line(data=df1, mapping=aes(x=Date, y=quadratic_model), inherit.aes = F, color=&quot;red&quot;) + labs(title = &quot;Mauna Loa Dataset (quadratic model)&quot;, y=&quot;CO2 (ppm)&quot;) Let’s see the residuals and the RMSE for this model sqrt(mean((df1$CO2 - CO2_quadratic_model$fitted.values)^2)) ## [1] 2.225216 df1 &lt;-df1 %&gt;% mutate(quadratic_model_resids = CO2 - CO2_quadratic_model$fitted.values) ggplot(data=df1, mapping=aes(x=Date, y=quadratic_model_resids)) + geom_line() + labs(title = &quot;Quadratic Model Residuals Fit&quot;, y=&quot;Residuals&quot;) Does this improve the fit? By how much have we shrunk the RMSE of residuals? It should be obvious from the simple visual inspection of the original series that there is a strong periodic component we are not currently capturing. It turns out to the a simple seasonal cycle: in spring/summer, the growth of terrestrial biomass in the Northern hemisphere sequesters CO2 away from the atmosphere, so the atmospheric CO2 concentrations goes down; the reverse happens in the Fall/Winter, where the fallen leaves are degraded and their organic carbon returned to mineral (CO2) form via respiration. 2.3.3 Harmonic waves Let us thus define harmonic waves (sines and cosines) with a period of 1 year and add them to the design matrix. xc&lt;-cos(2*pi*df1$Date) xs&lt;-sin(2*pi*df1$Date) fit.lm &lt;- lm(CO2~Date + I(Date^2) + xc + xs, data = df1) summary(fit.lm) ## ## Call: ## lm(formula = CO2 ~ Date + I(Date^2) + xc + xs, data = df1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.61346 -0.66340 -0.00194 0.61067 2.55221 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.042e+04 4.108e+02 122.72 &lt;2e-16 *** ## Date -5.191e+01 4.127e-01 -125.80 &lt;2e-16 *** ## I(Date^2) 1.344e-02 1.036e-04 129.75 &lt;2e-16 *** ## xc -9.994e-01 4.737e-02 -21.10 &lt;2e-16 *** ## xs 2.675e+00 4.729e-02 56.56 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9388 on 782 degrees of freedom ## Multiple R-squared: 0.9991, Adjusted R-squared: 0.9991 ## F-statistic: 2.185e+05 on 4 and 782 DF, p-value: &lt; 2.2e-16 sqrt(mean((df1$CO2 - fit.lm$fitted.values)^2)) ## [1] 0.9358151 df1 &lt;-df1 %&gt;% mutate(harmonic_model = fit.lm$fitted.values) ggplot(data=df1, mapping=aes(x=Date, y=CO2)) + geom_line() + geom_line(data=df1, mapping=aes(x=Date, y=harmonic_model), inherit.aes = F, color=&quot;red&quot;) + labs(title = &quot;Mauna Loa Dataset (quadratic + harmonic model)&quot;, y=&quot;CO2 (ppm)&quot;) And the residuals… df1 &lt;-df1 %&gt;% mutate(harmonic_model_resids = CO2 - fit.lm$fitted.values) ggplot(data=df1, mapping=aes(x=Date, y=harmonic_model_resids)) + geom_line() + labs(title = &quot;Quadratic+Harmonic Model Residuals Fit&quot;, y=&quot;Residuals&quot;) This should look better. Does it? By how much have we shrunk the RMSE of residuals (compared to the linear model)? 100*(1-sqrt(mean((df1$CO2 - fit.lm$fitted.values)^2))/sqrt(mean((df1$CO2 - CO2model$fitted.values)^2))) ## [1] 80.80033 80% improvement in the RMSE, that’s a big improvement! 2.3.4 Automated model The forecast package can figure this out for us. arima_model &lt;- forecast::auto.arima(df1$CO2, stepwise = F, approximation = F) summary(arima_model) ## Series: df1$CO2 ## ARIMA(4,1,1) with drift ## ## Coefficients: ## ar1 ar2 ar3 ar4 ma1 drift ## 1.4940 -0.8538 0.0971 -0.0886 -0.8898 0.1369 ## s.e. 0.0375 0.0645 0.0643 0.0369 0.0126 0.0077 ## ## sigma^2 = 0.4665: log likelihood = -814.05 ## AIC=1642.1 AICc=1642.24 BIC=1674.77 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set -0.0001132448 0.6799978 0.5402048 -0.003388041 0.1510635 0.4803613 ## ACF1 ## Training set -0.02630124 df1 &lt;-df1 %&gt;% mutate(arima_model = arima_model$fitted) ggplot(data=df1, mapping=aes(x=Date, y=CO2)) + geom_line() + geom_line(data=df1, mapping=aes(x=Date, y=arima_model), inherit.aes = F, color=&quot;red&quot;) + labs(title = &quot;Mauna Loa Dataset (arima_model)&quot;, y=&quot;CO2 (ppm)&quot;) This is an improvement on ou best RMSE with little work, but it’s a black box. This model is probably overfit, but we could turn some knobs to help prevent that if we spent some time with it. Let’s make some use of our newfound time series modeling skills. "],["forecasting.html", "2.4 Forecasting", " 2.4 Forecasting Let us return to our parametric model of atmospheric CO2 (quadractic trend + seasonal harmonics), which we may use to forecast future observations. Let’s extend our time axis to 2050 and see what CO2 may look like then if we don’t curb emissions or fail to remove it from the atmosphere: Date_ext &lt;- seq(max(df1$Date), length.out=length(df1$Date), by=1/12) df2 &lt;- df1 df2[,4] &lt;- Date_ext df2 &lt;- df2 %&gt;% mutate(CO2_predicted = predict(fit.lm, newdata = df2)) %&gt;% select(Date, CO2_predicted) ggplot(data=df1, mapping=aes(x=Date, y=CO2)) + geom_line() + geom_line(data=df2, mapping=aes(x=Date, y=CO2_predicted), inherit.aes = F, color=&quot;red&quot;) + labs(title = &quot;Mauna Loa CO2 forecast&quot;, y=&quot;CO2 (ppm)&quot;) "],["signal-processing-in-r.html", "3 Signal Processing in R", " 3 Signal Processing in R Often timeseries have to be treated prior to analysis. This is a delicate game: altering data is a cardinal sin, so one needs to be very aware of one’s doing prior to engaging in this activity. Here we show a few examples where such treatment may be warranted. Goals Become familiar with various filtering methods and some of their arguments Become familiar with various detrending methods and some of their arguments "],["packages.html", "3.1 Packages", " 3.1 Packages Let’s first load necessary packages: library(signal) library(zoo) library(trend) library(Mcomp) library(Rssa) library(tidyverse) "],["filtering.html", "3.2 Filtering", " 3.2 Filtering The goal of filtering is to enhance part of a signal, or suppress one or more parts. We’ll use R’s signal package, which provides a variety of filters. Let’s use data from a Hawai’i seismogram and isolate certain oscillations. df &lt;- read.table(&#39;https://github.com/LinkedEarth/PyRATES_practicums_py/raw/main/jbook/data/hawaii.e.dat&#39;, col.names = c(&#39;time&#39;, &#39;ground_motion&#39;)) df$time &lt;- df$time / 1000 df$ground_motion &lt;- df$ground_motion * 1000 ggplot(df, aes(x = time, y = ground_motion)) + geom_line() + labs(x = &#39;time [s]&#39;, y = &#39;amplitude (m)&#39;, title = &#39;Hawaii.e&#39;) 3.2.1 Filter Types First, let’s check if the data is evenly spaced: diff_time &lt;- data.frame(resolution= diff(df$time), time = rowMeans(cbind(df$time[-1],df$time[-length(df$time)]))) #make a histogram of the options ggplot(diff_time) + geom_line(aes(x = time,y = resolution)) + labs(x = &#39;time [s]&#39;, y = &#39;time difference [s]&#39;, title = &#39;Resolution&#39;) It appears that the resolution jumps at around the 100th second. Let’s interpolate to get an even resolution of 1ms: ts &lt;- zoo(df$ground_motion, df$time) ts_interp &lt;- na.approx(ts, xout = seq(min(df$time), max(df$time), by = 0.001)) ggplot() + geom_line(data = df, aes(x = time, y = ground_motion, color = &#39;original&#39;), alpha = 0.4) + geom_line(data = data.frame(time = index(ts_interp), ground_motion = coredata(ts_interp)), aes(x = time, y = ground_motion, color = &#39;interpolated&#39;), alpha = 0.4) + labs(x = &#39;time [s]&#39;, y = &#39;amplitude (m)&#39;, color = &#39;&#39;) The two series are indistinguishable, so we are safe to proceed. 3.2.1.1 Lowpass filter Let’s use a Butterworth filter with a cutoff_scale of 2 to remove all frequencies with a period of less than 2s. This is known as a low-pass filter. fs &lt;- 1000 # 1 kHz sampling rate f_nyq &lt;- fs / 2 # Nyquist frequency f_cutoff &lt;- 1/2 # 2s period # Create Butterworth filter butter_low &lt;- butter(2, f_cutoff / f_nyq, type = &quot;low&quot;) # Apply filter lp &lt;- filtfilt(butter_low, coredata(ts_interp)) ggplot() + geom_line(data = data.frame(time = index(ts_interp), ground_motion = coredata(ts_interp)), aes(x = time, y = ground_motion, color = &#39;original&#39;)) + geom_line(data = data.frame(time = index(ts_interp), ground_motion = lp), aes(x = time, y = ground_motion, color = &#39;low-pass&#39;)) + labs(x = &#39;time [s]&#39;, y = &#39;amplitude (m)&#39;, color = &#39;&#39;) 3.2.1.2 High-pass filter The opposite operation is a high-pass filter, which keeps only frequencies higher than a given cutoff. To generate a high-pass filter, you can simply subtract the low-pass filtered version from the original: hp &lt;- coredata(ts_interp) - lp ggplot() + geom_line(data = data.frame(time = index(ts_interp), ground_motion = coredata(ts_interp)), aes(x = time, y = ground_motion, color = &#39;original&#39;)) + geom_line(data = data.frame(time = index(ts_interp), ground_motion = hp), aes(x = time, y = ground_motion, color = &#39;high-pass&#39;), alpha = 0.8) + labs(x = &#39;time [s]&#39;, y = &#39;amplitude (m)&#39;, color = &#39;&#39;) 3.2.1.3 Band-pass filter If you want to isolate variability in a band of frequencies (say 1-2s), you can create a band-pass filter: f_low &lt;- 1/2 # 2s period f_high &lt;- 1/1 # 1s period butter_band &lt;- butter(2, c(f_low, f_high) / f_nyq, type = &quot;pass&quot;) bp &lt;- filtfilt(butter_band, coredata(ts_interp)) ggplot() + geom_line(data = data.frame(time = index(ts_interp), ground_motion = coredata(ts_interp)), aes(x = time, y = ground_motion, color = &#39;original&#39;)) + geom_line(data = data.frame(time = index(ts_interp), ground_motion = bp), aes(x = time, y = ground_motion, color = &#39;1-2s band-pass&#39;), alpha = 0.8) + labs(x = &#39;time [s]&#39;, y = &#39;amplitude (m)&#39;, color = &#39;&#39;) 3.2.1.4 Notch filter Conversely, if you wanted to remove all variability between 1-2s (a notch in frequency space), you would subtract the bandpass-filtered version. notch &lt;- coredata(ts_interp) - bp ggplot() + geom_line(data = data.frame(time = index(ts_interp), ground_motion = coredata(ts_interp)), aes(x = time, y = ground_motion, color = &#39;original&#39;)) + geom_line(data = data.frame(time = index(ts_interp), ground_motion = notch), aes(x = time, y = ground_motion, color = &#39;1-2s notch&#39;), alpha = 0.8) + labs(x = &#39;time [s]&#39;, y = &#39;amplitude (m)&#39;, color = &#39;&#39;) "],["detrending.html", "3.3 Detrending", " 3.3 Detrending Now let’s move on to detrending. We’ll use the HadCRUT5 global mean surface temperature dataset. gmst &lt;- read_csv(&quot;https://github.com/LinkedEarth/Pyleoclim_util/raw/master/pyleoclim/data/HadCRUT.5.0.1.0.analysis.summary_series.global.annual.csv&quot;) %&gt;% select(time = Time,temp = `Anomaly (deg C)`) ggplot(gmst, aes(x = time, y = temp)) + geom_line() + labs(x = &#39;Time [year C.E.]&#39;, y = &#39;GMST [°C]&#39;) 3.3.1 Detrending methods in R Let’s apply 3 methods: linear detrending, Empirical Mode Decomposition (EMD), and Savitzky-Golay filter: # Linear detrending gmstLinear &lt;- gmst gmstLinear$detrended &lt;- pracma::detrend(gmstLinear$temp) gmstLinear$trend &lt;- gmstLinear$temp - gmstLinear$detrended gmstLinear$method &lt;- &quot;Linear&quot; # EMD using the EMD package library(EMD) emd_result &lt;- emd(gmst$temp, boundary=&#39;symmetric&#39;, max.imf=4) gmstEmd &lt;- gmst gmstEmd$detrended &lt;- gmst$temp-emd_result$residue gmstEmd$trend &lt;- gmstEmd$temp - gmstEmd$detrended gmstEmd$method &lt;- &quot;EMD&quot; # Savitzky-Golay filter gmstSg &lt;- gmst gmstSg$trend &lt;- signal::sgolayfilt(gmst$temp, p = 3, n = 51) gmstSg$detrended &lt;- gmst$temp - gmstSg$trend gmstSg$method &lt;- &quot;Savitzky-Golay&quot; #prepare the data for plotting gmstToPlot &lt;- dplyr::bind_rows(gmstLinear,gmstEmd,gmstSg) %&gt;% tidyr::pivot_longer(cols = -c(time,method),values_to = &quot;Temperature&quot;,names_to = &quot;options&quot;) %&gt;% mutate(method = factor(method,levels = c(&quot;Linear&quot;,&quot;EMD&quot;,&quot;Savitzky-Golay&quot;))) # Plot it! ggplot(gmstToPlot) + geom_line(aes(x = time, y = Temperature, color = options)) + facet_grid(method ~ .) The linear trend here does a decent job at capturing first-order behavior. The EMD (approximated by SSA) and Savitzky-Golay methods also capture the nonlinear trend. 3.3.2 SSA detrending Another option to isolate a non-linear trend is Singular Spectrum Analysis (SSA). If there is a prominent trend, it is often the first mode coming out of that analysis. gmst_ssa &lt;- ssa(gmst$temp, L = 10) plot(gmst_ssa) The first mode accounts for most fraction of the variance. Let’s compute this: paste0(round(gmst_ssa$sigma[1] / sum(gmst_ssa$sigma),3) * 100,&quot;%&quot;) ## [1] &quot;55%&quot; Let’s use this mode as the trend: ssa_trend &lt;- reconstruct(gmst_ssa, groups = list(trend = 1)) gmst_ssa_dtd &lt;- data.frame(time = gmst$time, temp = gmst$temp - ssa_trend$trend) ggplot(gmst, aes(x = time, y = temp)) + geom_line() + geom_line(aes(y = ssa_trend$trend, color = &quot;SSA trend&quot;), alpha = 0.8) + geom_line(aes(y = gmst_ssa_dtd$temp, color = &quot;SSA detrended&quot;), alpha = 0.8) + labs(x = &#39;Time [year C.E.]&#39;, y = &#39;GMST [°C]&#39;, color = &#39;&#39;) This pre-processing allows us to better isolate oscillatory behavior. To see this, let’s look at the spectra of the original and detrended versions: library(astrochron) mtmOrig &lt;- astrochron::mtm(dat = gmst,output = 1,verbose = F,genplot = F) %&gt;% select(Frequency,Power) %&gt;% mutate(data = &quot;Original&quot;, period = (1/Frequency)) mtmSSA &lt;- astrochron::mtm(dat = gmst_ssa_dtd,output = 1,verbose = F,genplot = F) %&gt;% select(Frequency,Power) %&gt;% mutate(data = &quot;SSA-detrended&quot;, period = (1/Frequency)) library(scales) reverselog_trans &lt;- function(base = exp(1)) { trans &lt;- function(x) -log(x, base) inv &lt;- function(x) base^(-x) trans_new(paste0(&quot;reverselog-&quot;, format(base)), trans, inv, log_breaks(base = base), domain = c(1e-100, Inf)) } bind_rows(mtmOrig,mtmSSA) %&gt;% ggplot() + geom_line(aes(x = period,y = Power)) + scale_x_continuous(trans = reverselog_trans(10),breaks = c(100,50,20,10,5,2),limits = c(100,2)) + scale_y_log10() + facet_grid(data ~ .) We see that detrending has removed much of the variability at scales longer than ~30y, allowing us to hone in on various peaks near 3.5, 6, 10, and 20 years. To see if those are significant, however, you would need to apply a significance test, which we would cover in another tutorial. "],["takeways.html", "3.4 Takeways", " 3.4 Takeways R provides powerful packages for signal processing, including filtering and detrending. Interactive visualization is a key part of the process, to make sure that the signal processing is achieving what it is supposed to do. "],["association-correlation-and-significance-in-r.html", "4 Association: Correlation and Significance in R ", " 4 Association: Correlation and Significance in R "],["measures-of-association-nao-vs-soi.html", "4.1 Measures of association: NAO vs SOI", " 4.1 Measures of association: NAO vs SOI In this notebook we explore the relationship between two well-known climate indices, the Southern Oscillation Index (SOI) and the North Atlantic Oscillation (NAO) index. The NAO and SOI index have been alleged to show some relationship, albeit a subtle one, in some seasons/epochs. Specifically, we will explore: effects of trends effects of autocorrelation various measures of association various methods of establishing the significance of a relationship 4.1.1 R packages We will use tidyverse for data formating/manipulation as well as for plotting We will also make use of reshape2 for data formatting astrochron will provide a test of correlation significance library(reshape2) library(tidyverse) library(astrochron) 4.1.2 Data Wrangling The NAO data are from NCEP The SOI data ship with Pyleoclim, which houses a few datasets that make it easy to experiment with real-world geoscientific timeseries. We’ll grab the data directly from the github page. #Load SOI SOI &lt;- read.table(&quot;https://github.com/LinkedEarth/Pyleoclim_util/raw/master/pyleoclim/data/soi_data.csv&quot;, sep = &quot;,&quot;, header = TRUE, skip = 1) head(SOI) ## Date Year Value ## 1 195101 1951.000 1.5 ## 2 195102 1951.083 0.9 ## 3 195103 1951.167 -0.1 ## 4 195104 1951.250 -0.3 ## 5 195105 1951.333 -0.7 ## 6 195106 1951.417 0.2 #Load NAO NAO &lt;- read.table(&#39;https://www.cpc.ncep.noaa.gov/products/precip/CWlink/pna/norm.nao.monthly.b5001.current.ascii.table&#39;,header = TRUE, fill=TRUE, row.names = NULL) head(NAO) ## row.names Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1 1950 0.92 0.40 -0.36 0.73 -0.59 -0.06 -1.26 -0.05 0.25 0.85 -1.26 -1.02 ## 2 1951 0.08 0.70 -1.02 -0.22 -0.59 -1.64 1.37 -0.22 -1.36 1.87 -0.39 1.32 ## 3 1952 0.93 -0.83 -1.49 1.01 -1.12 -0.40 -0.09 -0.28 -0.54 -0.73 -1.13 -0.43 ## 4 1953 0.33 -0.49 -0.04 -1.67 -0.66 1.09 0.40 -0.71 -0.35 1.32 1.04 -0.47 ## 5 1954 0.37 0.74 -0.83 1.34 -0.09 -0.25 -0.60 -1.90 -0.44 0.60 0.40 0.69 ## 6 1955 -1.84 -1.12 -0.53 -0.42 -0.34 -1.10 1.76 1.07 0.32 -1.47 -1.29 0.17 names(NAO)[1] &lt;- &quot;Year&quot; Note the latter arguments to read.table(). fill=TRUE adds NA values where data is missing at the end of the file while row.names=NULL employs the row names of the file as the first column of the data.frame. 4.1.3 Format and plot the NAO data The melt() function from reshape2 is handy for reorganizing data from wide to long format. In this case we have a 13-column data.frame with a Year column and a column for each month. melt() reformats this data to 3 columns: Year, month, and value. Next we use lubridate to convert the Year+month to a datetime object. The data are provided in monthly averages, so we assign the values to the 15th of each month.We utilize dplyr to format the data for plotting. ## datetime value ## 1 1950-01-15 0.92 ## 2 1950-02-15 0.40 ## 3 1950-03-15 -0.36 ## 4 1950-04-15 0.73 ## 5 1950-05-15 -0.59 ## 6 1950-06-15 -0.06 Now we use ggplot to look at the timeseries ggplot(NAO, aes(x=datetime, y=value)) + geom_line() + labs(title = &quot;North Atlantic Oscillation&quot;, y=&quot;Index&quot;, x=&quot;Year&quot;) + theme_minimal() 4.1.4 Merge the SOI data into the NAO data.frame Again we use lubridate to format the datetime portion. Next, we create a new data.frame with evenly space time using seq. Our final data.frame will merge the 3 unique datetime series from NAO, SOI, and the evenly spaced series into a single column. Let’s take a look at the top and bottom of the new data.frame SOI &lt;- SOI %&gt;% mutate(datetime = as.Date(format(date_decimal(Year), &quot;%Y-%m-%d&quot;))) %&gt;% rename(SOI = Value) %&gt;% dplyr::select(datetime, SOI) head(SOI) ## datetime SOI ## 1 1951-01-01 1.5 ## 2 1951-01-31 0.9 ## 3 1951-03-02 -0.1 ## 4 1951-04-02 -0.3 ## 5 1951-05-02 -0.7 ## 6 1951-06-02 0.2 newDateDF &lt;- data.frame(datetime = as.Date(round(seq(as.numeric(min(SOI$datetime)), as.numeric(max(SOI$datetime)), length.out=69*12),5))) SOInewDate &lt;- merge.data.frame(SOI, newDateDF, all = T) dfAll &lt;- merge.data.frame(NAO, SOInewDate, all = T) #first 20 head(dfAll,n = 20) ## datetime value SOI ## 1 1950-01-15 0.92 NA ## 2 1950-02-15 0.40 NA ## 3 1950-03-15 -0.36 NA ## 4 1950-04-15 0.73 NA ## 5 1950-05-15 -0.59 NA ## 6 1950-06-15 -0.06 NA ## 7 1950-07-15 -1.26 NA ## 8 1950-08-15 -0.05 NA ## 9 1950-09-15 0.25 NA ## 10 1950-10-15 0.85 NA ## 11 1950-11-15 -1.26 NA ## 12 1950-12-15 -1.02 NA ## 13 1951-01-01 NA 1.5 ## 14 1951-01-15 0.08 NA ## 15 1951-01-31 NA 0.9 ## 16 1951-02-15 0.70 NA ## 17 1951-03-02 NA -0.1 ## 18 1951-03-15 -1.02 NA ## 19 1951-04-02 NA -0.3 ## 20 1951-04-15 -0.22 NA #last 20 tail(dfAll,n = 20) ## datetime value SOI ## 2181 2023-05-15 0.39 NA ## 2182 2023-06-15 -0.58 NA ## 2183 2023-07-15 -2.17 NA ## 2184 2023-08-15 -1.16 NA ## 2185 2023-09-15 -0.44 NA ## 2186 2023-10-15 -2.03 NA ## 2187 2023-11-15 -0.32 NA ## 2188 2023-12-15 1.94 NA ## 2189 2024-01-15 0.21 NA ## 2190 2024-02-15 1.09 NA ## 2191 2024-03-15 -0.21 NA ## 2192 2024-04-15 -0.78 NA ## 2193 2024-05-15 -0.44 NA ## 2194 2024-06-15 NA NA ## 2195 2024-07-15 NA NA ## 2196 2024-08-15 NA NA ## 2197 2024-09-15 NA NA ## 2198 2024-10-15 NA NA ## 2199 2024-11-15 NA NA ## 2200 2024-12-15 NA NA 4.1.5 Interpolation Now we will use the evenly space datetime to interpolate NAO and SOI: we restrict our time interval to that with data from both sources we perform linear interpolation of each index we extract the data for only the interpolated values, creating evenly spaced series dfAll &lt;- dfAll %&gt;% slice(13:2505) %&gt;% mutate(NAO = approx(datetime, value, datetime)$y) %&gt;% mutate(SOI = approx(datetime, SOI, datetime)$y) %&gt;% select(-value) %&gt;% slice(which(datetime %in% newDateDF$datetime)) %&gt;% drop_na() head(dfAll) ## datetime SOI NAO ## 1 1951-01-31 0.9 0.40000000 ## 2 1951-03-02 -0.1 -0.22142857 ## 3 1951-04-02 -0.3 -0.55548387 ## 4 1951-05-02 -0.7 -0.42966667 ## 5 1951-06-02 0.2 -1.19967742 ## 6 1951-07-02 -1.0 0.06566667 allLong &lt;- melt(dfAll,id.vars = &quot;datetime&quot;) head(allLong) ## datetime variable value ## 1 1951-01-31 SOI 0.9 ## 2 1951-03-02 SOI -0.1 ## 3 1951-04-02 SOI -0.3 ## 4 1951-05-02 SOI -0.7 ## 5 1951-06-02 SOI 0.2 ## 6 1951-07-02 SOI -1.0 Now we can convert to a long format for plotting allLong &lt;- melt(dfAll,id.vars = &quot;datetime&quot;) head(allLong) ## datetime variable value ## 1 1951-01-31 SOI 0.9 ## 2 1951-03-02 SOI -0.1 ## 3 1951-04-02 SOI -0.3 ## 4 1951-05-02 SOI -0.7 ## 5 1951-06-02 SOI 0.2 ## 6 1951-07-02 SOI -1.0 ggplot(allLong, aes(x=datetime, y=value, group=variable)) + geom_line() + facet_wrap(~variable, ncol=1) + labs(title = &quot;NAO vs SOI (Interpolated)&quot;, y=&quot;Index&quot;, x=&quot;Year&quot;) + theme_minimal() 4.1.6 Correlation Both calls use lapply to repeat a correlation test 3 times using different methods: “pearson”, “spearman”, and “kendall” The second call employs a Monte Carlo simulation from the astrochron package. The method implemented is described in Ebisuzaki (1997). lapply(c(&quot;pearson&quot;, &quot;spearman&quot;, &quot;kendall&quot;), function(x) cor.test(dfAll$NAO, dfAll$SOI, method = x)) ## [[1]] ## ## Pearson&#39;s product-moment correlation ## ## data: dfAll$NAO and dfAll$SOI ## t = -1.1793, df = 353, p-value = 0.2391 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.16565406 0.04171388 ## sample estimates: ## cor ## -0.06264618 ## ## ## [[2]] ## ## Spearman&#39;s rank correlation rho ## ## data: dfAll$NAO and dfAll$SOI ## S = 8004173, p-value = 0.1673 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## -0.07346052 ## ## ## [[3]] ## ## Kendall&#39;s rank correlation tau ## ## data: dfAll$NAO and dfAll$SOI ## z = -1.3688, p-value = 0.1711 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## -0.04938327 #Methods: 1-pearson, 2-spearman, 3-kendall lapply(c(1,2,3), function(x) surrogateCor(dfAll$NAO,dfAll$SOI,nsim = 10000,cormethod = x, genplot = F, verbose = F)) ## [[1]] ## datcor pvalue ## 1 -0.06264618 0.3078 ## ## [[2]] ## datcor pvalue ## 1 -0.07346052 0.2314 ## ## [[3]] ## datcor pvalue ## 1 -0.04938327 0.2363 All the methods tested show similarly weak evidence for significant correlations. "],["spurious-correlations.html", "4.2 Spurious Correlations", " 4.2 Spurious Correlations In the geosciences, there are two process that might artificially increase correlations between otherwise unrelated variables 4.2.1 Smoothing common trends Smoothing-enhanced correlations dfAll$lowpassNAO &lt;- smooth.spline(x = dfAll$datetime, y=dfAll$NAO, spar = 0.2)$y dfAll$lowpassSOI &lt;- smooth.spline(x = dfAll$datetime, y=dfAll$SOI, spar = 0.2)$y Let’s reformat and plot the smoothed series allLong2 &lt;- melt(dfAll,id.vars = &quot;datetime&quot;) allLong2 &lt;- allLong2 %&gt;% mutate(group = ifelse(grepl(&quot;SOI&quot;, variable), &quot;SOI&quot;, &quot;NAO&quot;)) %&gt;% mutate(type = ifelse(grepl(&quot;lowpass&quot;, variable), &quot;filtered&quot;, &quot;original&quot;)) head(allLong2) ## datetime variable value group type ## 1 1951-01-31 SOI 0.9 SOI original ## 2 1951-03-02 SOI -0.1 SOI original ## 3 1951-04-02 SOI -0.3 SOI original ## 4 1951-05-02 SOI -0.7 SOI original ## 5 1951-06-02 SOI 0.2 SOI original ## 6 1951-07-02 SOI -1.0 SOI original ggplot(allLong2, aes(x=datetime, y=value, group=group, color=type)) + geom_line() + facet_wrap(~group, ncol=1) + labs(title = &quot;NAO vs SOI&quot;, y=&quot;Index&quot;, x=&quot;Year&quot;) + theme_minimal() Perhaps the smoothed series will show the cryptic relationship lapply(c(&quot;pearson&quot;, &quot;spearman&quot;, &quot;kendall&quot;), function(x) cor.test(dfAll$lowpassNAO, dfAll$lowpassSOI, method = x)) ## [[1]] ## ## Pearson&#39;s product-moment correlation ## ## data: dfAll$lowpassNAO and dfAll$lowpassSOI ## t = -2.4976, df = 353, p-value = 0.01296 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.23267352 -0.02807391 ## sample estimates: ## cor ## -0.1317769 ## ## ## [[2]] ## ## Spearman&#39;s rank correlation rho ## ## data: dfAll$lowpassNAO and dfAll$lowpassSOI ## S = 8059524, p-value = 0.1282 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## -0.08088386 ## ## ## [[3]] ## ## Kendall&#39;s rank correlation tau ## ## data: dfAll$lowpassNAO and dfAll$lowpassSOI ## z = -1.6019, p-value = 0.1092 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## -0.0569587 lapply(c(1,2,3), function(x) surrogateCor(dfAll$lowpassNAO, dfAll$lowpassSOI, nsim = 10000, cormethod = x, genplot = F, verbose = F) ) ## [[1]] ## datcor pvalue ## 1 -0.1317769 0.276 ## ## [[2]] ## datcor pvalue ## 1 -0.08088386 0.49 ## ## [[3]] ## datcor pvalue ## 1 -0.0569587 0.4754 Okay, so the simple Pearson correlation comes through significant (p &lt; .05), but this assumes that each value is independent, which we know is not true because we smoothed the series. The Ebisuzaki test is very useful here, and we see that the corresponding Pearson p-value does not approach .05. Take-home message: common trends can easily create the appearance of correlations (see Tyler Vigen’s excellent website) and really complicate assessments of significance. If the trend is not relevant to your question, we recommend removing it prior to computing correlations, e.g. using lm(). "],["takeways-1.html", "4.3 Takeways", " 4.3 Takeways Not only is correlation not indicative of causation, but spurious correlations abound, often driven by smoothing, trends, or short sample sizes. Some null hypotheses are more stringent than others, but the simple methods like cor.test() assume independent samples, which is hardly ever verified in the geosciences. Make sure you carefully match your null hypothesis to your data/problem. "],["references.html", "4.4 References", " 4.4 References Ebisuzaki, W. (1997). A method to estimate the statistical significance of a correlation when the data are serially correlated. Journal of climate, 10(9), 2147-2153. "],["spectral-analysis-in-r.html", "5 Spectral Analysis in R", " 5 Spectral Analysis in R Recall that in spectral analysis we step into a “bizarro world”, switching from the time domain to the frequency domain. There, two essential features are of interest: peaks, and background. In this tutorial, we will perform spectral analysis on Rio Grande streamflow, and see if we can learn anything interesting about it in this way. 5.0.1 Packages library(tidyverse) library(scales) library(astrochron) library(lomb) library(biwavelet) library(scales) 5.0.2 Dataset We will be looking at the daily discharge of the Rio Grande, which has been measured at Embudo, NM since 1889. The data and their associated metadata may be retrieved from the USGS website. Let’s load them and have a look: df &lt;- read.table(&#39;https://waterdata.usgs.gov/nwis/dv?cb_00060=on&amp;format=rdb&amp;site_no=08279500&amp;legacy=&amp;referred_module=sw&amp;period=&amp;begin_date=1889-01-01&amp;end_date=2024-05-20&#39;, skip=35, sep=&quot;\\t&quot;) names(df) &lt;- c(&#39;agency&#39;, &#39;site_no&#39;, &#39;datetime&#39;,&#39;discharge (cf/s)&#39;,&#39;code&#39;) df$datetime &lt;- lubridate::as_datetime(df$datetime) head(df) ## agency site_no datetime discharge (cf/s) code ## 1 USGS 8279500 1889-01-05 413 A ## 2 USGS 8279500 1889-01-06 408 A ## 3 USGS 8279500 1889-01-07 410 A ## 4 USGS 8279500 1889-01-08 405 A ## 5 USGS 8279500 1889-01-09 379 A ## 6 USGS 8279500 1889-01-10 418 A First, let us inspect the distribution of values: hist(df$`discharge (cf/s)`, main = &quot;Distribution of Rio Grande Discharge&quot;, xlab = &quot;Discharge (cf/s)&quot;) Like many other variables, streamflow is positively skewed: the distribution is asymmetric, with most values near zero and few instances of very high streamflow. Now let’s inspect the temporal behavior: ggplot(df, aes(x = datetime, y = `discharge (cf/s)`)) + geom_line() + labs(x = &quot;Time&quot;, y = &quot;Discharge (cf/s)&quot;) "],["aggregate-to-monthly.html", "5.1 Aggregate to monthly", " 5.1 Aggregate to monthly discharge_monthly &lt;- df |&gt; group_by(Date = floor_date(ymd(df$datetime), &#39;1 month&#39;)) |&gt; summarise(discharge = mean(`discharge (cf/s)`, na.rm = TRUE), .groups = &#39;drop&#39;) ggplot(discharge_monthly, aes(x=Date, y=discharge)) + labs(title = &quot;Rio Grande at Embudo, NM (monthly)&quot;, x=&quot;Year (CE)&quot;, y=&quot;dicharge (cf/s)&quot;) + geom_line() + ggtitle(&quot;Rio Grand Discharge&quot;) + theme_light() 5.1.1 Even sampling missing_vals &lt;- discharge_monthly$Date[which(is.na(discharge_monthly$discharge))] missing_vals ## [1] &quot;1894-03-01&quot; &quot;1894-04-01&quot; &quot;1894-05-01&quot; &quot;1894-06-01&quot; &quot;1894-07-01&quot; &quot;1894-08-01&quot; ## [7] &quot;1894-09-01&quot; &quot;1904-04-01&quot; &quot;1904-05-01&quot; &quot;1904-06-01&quot; &quot;1904-07-01&quot; &quot;1904-08-01&quot; ## [13] &quot;1904-09-01&quot; &quot;1904-10-01&quot; &quot;1904-11-01&quot; &quot;1904-12-01&quot; &quot;1905-01-01&quot; &quot;1905-02-01&quot; ## [19] &quot;1905-03-01&quot; &quot;1905-04-01&quot; &quot;1905-05-01&quot; &quot;1905-06-01&quot; &quot;1905-07-01&quot; &quot;1905-08-01&quot; ## [25] &quot;1905-09-01&quot; &quot;1905-10-01&quot; &quot;1905-11-01&quot; &quot;1905-12-01&quot; &quot;1906-01-01&quot; &quot;1906-02-01&quot; ## [31] &quot;1906-03-01&quot; &quot;1906-04-01&quot; &quot;1906-05-01&quot; &quot;1906-06-01&quot; &quot;1906-07-01&quot; &quot;1906-08-01&quot; ## [37] &quot;1906-09-01&quot; &quot;1906-10-01&quot; &quot;1906-11-01&quot; &quot;1906-12-01&quot; &quot;1907-01-01&quot; &quot;1907-02-01&quot; ## [43] &quot;1907-03-01&quot; &quot;1907-04-01&quot; &quot;1907-05-01&quot; &quot;1907-06-01&quot; &quot;1907-07-01&quot; &quot;1907-08-01&quot; ## [49] &quot;1907-09-01&quot; &quot;1907-10-01&quot; &quot;1907-11-01&quot; &quot;1907-12-01&quot; &quot;1908-01-01&quot; &quot;1908-02-01&quot; ## [55] &quot;1908-03-01&quot; &quot;1908-04-01&quot; &quot;1908-05-01&quot; &quot;1908-06-01&quot; &quot;1908-07-01&quot; &quot;1908-08-01&quot; ## [61] &quot;1908-09-01&quot; &quot;1908-10-01&quot; &quot;1908-11-01&quot; &quot;1908-12-01&quot; &quot;1909-01-01&quot; &quot;1909-02-01&quot; ## [67] &quot;1909-03-01&quot; &quot;1909-04-01&quot; &quot;1909-05-01&quot; &quot;1909-06-01&quot; &quot;1909-07-01&quot; &quot;1909-08-01&quot; ## [73] &quot;1909-09-01&quot; &quot;1909-10-01&quot; &quot;1909-11-01&quot; &quot;1909-12-01&quot; &quot;1910-01-01&quot; &quot;1910-02-01&quot; ## [79] &quot;1910-03-01&quot; &quot;1910-04-01&quot; &quot;1910-05-01&quot; &quot;1910-06-01&quot; &quot;1910-07-01&quot; &quot;1910-08-01&quot; ## [85] &quot;1910-09-01&quot; &quot;1910-10-01&quot; &quot;1910-11-01&quot; &quot;1910-12-01&quot; &quot;1911-01-01&quot; &quot;1911-02-01&quot; ## [91] &quot;1911-03-01&quot; &quot;1911-04-01&quot; &quot;1911-05-01&quot; &quot;1911-06-01&quot; &quot;1911-07-01&quot; &quot;1911-08-01&quot; ## [97] &quot;1911-09-01&quot; &quot;1911-10-01&quot; &quot;1911-11-01&quot; &quot;1911-12-01&quot; &quot;1912-01-01&quot; &quot;1912-02-01&quot; ## [103] &quot;1912-03-01&quot; &quot;1912-04-01&quot; &quot;1912-05-01&quot; &quot;1912-06-01&quot; &quot;1912-07-01&quot; &quot;1912-08-01&quot; df3 &lt;- discharge_monthly |&gt; dplyr::filter(Date &gt; max(missing_vals)) hist(as.numeric(diff(df3$Date)),main = &quot;Distribution of Time Steps&quot;, xlab = &quot;Days&quot;) df4 &lt;- df3 |&gt; mutate(Date = decimal_date(Date)) |&gt; astrochron::linterp(dt=(1/12),genplot = F) |&gt; dplyr::filter(Date &gt; max(missing_vals)) ## ## ----- APPLYING PIECEWISE-LINEAR INTERPOLATION TO STRATIGRAPHIC SERIES ----- ## ## * Number of samples= 1341 ## * New number of samples= 1340 ggplot(df4, aes(x=Date, y=discharge)) + labs(title = &quot;Rio Grande at Embudo, NM (30.4375-day period)&quot;, x=&quot;Year (CE)&quot;, y=&quot;dicharge (cf/s)&quot;) + geom_line() + theme_light() 5.1.2 multi-taper mtm1 &lt;- mtm(df4,output = 1,verbose = F) |&gt; mutate(Period = 1/Frequency, Power = Power/1000) |&gt; #account for differing units in astrochrons MTM dplyr::select(Period, Power) reverselog_trans &lt;- function(base = exp(1)) { trans &lt;- function(x) -log(x, base) inv &lt;- function(x) base^(-x) trans_new(paste0(&quot;reverselog-&quot;, format(base)), trans, inv, log_breaks(base = base), domain = c(1e-100, Inf)) } ggplot(mtm1, aes(x=Period, y=Power)) + labs(title = &quot;Rio Grande discharge spectrum (mtm)&quot;) + geom_line() + scale_y_log10() + scale_x_continuous(trans = reverselog_trans(10), breaks = c(100,50,20,10,5,2,1,0.5,0.2), limits = c(100,0.2)) + theme_light() The prominent feature is a strong annual cycle and higher-order harmonics, super-imposed on a “warm colored” background (i.e. variations at long timescales are larger than variations at short timescales). There are hints of long-term scaling as well, one in the subannual range, and one from period of 1 to 50y. We may be interested in the shape of this background, and whether it can be fit by one or more power laws. In addition, we may be interested in interannual (year-to-year) to interdecadal (decade-to-decade) variations in streamflow. We would like to know whether the peak observed around 3-4 years in the plot above is significant with respect to a reasonable null. "],["stl.html", "5.2 STL", " 5.2 STL To do both of these things it would make sense to remove the annual cycle. We’ll use STL decomposition for this. 5.2.1 Gaussianize Now, because of the positive skew mentioned above, it turns out that the STL decomposition has trouble with this dataset. Instead, we can use the gaussianize() to map this dataset to a standard normal. There are applications for which this would be undesirable, but for this purpose it is good: Gaussianize is available as part of the geoChronR package - but it’s also pretty simple, so let’s just define that here: gaussianize &lt;- function(X) { if (!is.matrix(X)) { X = as.matrix(X) } p = NCOL(X) n = NROW(X) Xn = matrix(data = NA, nrow = n, ncol = p) for (j in 1:p) { nz &lt;- which(!is.na(X[, j])) N &lt;- length(nz) R &lt;- rank(X[nz, j]) CDF &lt;- R/N - 1/(2 * N) Xn[nz, j] &lt;- qnorm(CDF) } return(Xn) } 5.2.2 MTM, again Let’s take a look at how this affects the spectrum: df5 &lt;- df4 |&gt; mutate(discharge = gaussianize(discharge)) mtm2 &lt;- mtm(df5,output = 1,verbose = F) |&gt; mutate(Period = 1/Frequency, #Calculate frequency Power = Power/1000, #account for differing units in astrochrons MTM AR1_95_power = AR1_95_power/1000) |&gt; #account for differing units in astrochrons MTM dplyr::select(Period, Power) ggplot(mtm2, aes(x=Period, y=Power)) + labs(title = &quot;Rio Grande discharge spectrum (mtm)&quot;) + geom_line() + scale_y_log10() + scale_x_continuous(trans = reverselog_trans(10), breaks = c(100,50,20,10,5,2,1,0.5,0.2), limits = c(100,0.2)) + theme_light() 5.2.3 Applying STL We see that the spectrum is nearly unchanged, but now we can apply STL: dis &lt;- ts(as.numeric(df5$discharge), start = c(1889, 1), frequency = 12) stl_res &lt;- stl(dis, s.window = &quot;periodic&quot;) plot(stl_res) 5.2.4 The trend Now let’s apply spectral analysis to the trend component: df6 &lt;- df5 |&gt; mutate(discharge = as.numeric(stl_res$time.series[,2])) mtm3 &lt;- mtm(df6,output = 1,verbose = F,) |&gt; mutate(Period = 1/Frequency, Power = Power/1000, #account for differing units in astrochrons MTM AR1_95_power = AR1_95_power/1000) #account for differing units in astrochrons MTM trendOnlySpectrum &lt;- ggplot(mtm3, aes(x=Period, y=Power)) + labs(title = &quot;Rio Grande discharge trend-only spectrum (mtm)&quot;) + geom_line() + geom_line(aes(y = AR1_95_power),color = &quot;red&quot;) + scale_y_log10() + scale_x_continuous(trans = reverselog_trans(10), breaks = c(100,50,20,10,5,2,1,0.5,0.2), limits = c(100,0.2)) + theme_light() trendOnlySpectrum Indeed this removed the annual cycle and its harmonics (for the most part). It’s a fairly aggressive treatment, but we can now compare the significance of the interannual peaks w.r.t to an AR(1) benchmark. We see that the broadband peak in the 2-10y range is above the AR(1) spectrum, so these frequencies could be considered significant. "],["estimation-of-scaling-behavior.html", "5.3 Estimation of scaling behavior", " 5.3 Estimation of scaling behavior In this last example, we fit a single scaling exponent to the timeseries, spanning the entire range of frequencies (periods). Under the hood, all we are doing is fitting a straight line to the spectrum: # Log-transform both axes toScale &lt;- dplyr::filter(mtm3, between(Period,0.2,100)) log_period &lt;- log10(toScale$Period) log_spec &lt;- log10(toScale$Power) # Fit a line fit &lt;- lm(log_spec ~ log_period) toScale$plotLine &lt;- predict(fit) # Plot trendOnlySpectrum + geom_line(data = toScale,aes(x = Period,y = 10^plotLine),color = &quot;blue&quot;,linetype = 3) This results in a fairly steep exponent near 4.52. If we were specifically interested in the scaling exponent (spectral slope) between periods of 2-100y, you would do it like so: # Log-transform both axes toScale &lt;- dplyr::filter(mtm3, between(Period,2,100)) log_period &lt;- log10(toScale$Period) log_spec &lt;- log10(toScale$Power) # Fit a line fitLong &lt;- lm(log_spec ~ log_period) toScale$plotLine &lt;- predict(fitLong) # Plot trendOnlySpectrum + geom_line(data = toScale,aes(x = Period,y = 10^plotLine),color = &quot;blue&quot;,linetype = 3) We see that this results in a much flatter line, i.e. a smaller scaling exponent around 0.52. "],["gap-tolerant-spectral-analysis.html", "5.4 Gap-tolerant spectral analysis", " 5.4 Gap-tolerant spectral analysis 5.4.1 Lomb-Scargle We return to the original series and apply a technique to obtain the spectrum, keeping gaps in the series, known as the Lomb-Scargle periodogram: #Let&#39;s average (or bin) the data into monthly intervals dfBinned &lt;- df |&gt; mutate(year = year(datetime), month = month(datetime)) |&gt; group_by(year,month) |&gt; summarize(discharge = mean(`discharge (cf/s)`,na.rm = TRUE)) |&gt; mutate(yearDecimal = year + month/12 - 1/24) |&gt; dplyr::filter(is.finite(discharge)) ## `summarise()` has grouped output by &#39;year&#39;. You can override using the `.groups` ## argument. # Compute Lomb-Scargle periodogram lomb &lt;- lomb::lsp(x = dfBinned$discharge, times = dfBinned$yearDecimal, ofac = 4, scaleType = &quot;period&quot;,normalize = &quot;press&quot;, plot = FALSE) ltp &lt;- data.frame(Period = 1/lomb$scanned, Power = lomb$power) # Plot ggplot(ltp, aes(x=Period, y=Power)) + labs(title = &quot;Rio Grande discharge Lomb-Scargle spectrum&quot;) + geom_line() + #geom_line(aes(y = AR1_95_power),color = &quot;red&quot;) + scale_y_log10() + scale_x_continuous(trans = reverselog_trans(10), breaks = c(100,50,20,10,5,2,1,0.5,0.2), limits = c(100,0.2)) + theme_light() We can see that this resembles our interpolated MTM approach - but has some differences. Let’s plot on the same graph to take a closer look: comboPlotData &lt;- bind_rows(mutate(mtm1,method = &quot;mtm&quot;), mutate(ltp,method = &quot;lomb-scargle&quot;)) ggplot(comboPlotData, aes(x=Period, y=Power,color = method)) + labs(title = &quot;Rio Grande discharge spectrum&quot;) + geom_line() + scale_y_log10() + scale_x_continuous(trans = reverselog_trans(10), breaks = c(100,50,20,10,5,2,1,0.5,0.2), limits = c(100,0.2)) + theme_light() It is often useful to be able to compare methods and/or parameter choices, to see if the results are robust. We see that some choices lump peaks into broad bands, others tend to slice them up. 5.4.2 Wavelet Wavelet analysis using the Morlet wavelet: # Convert to matrix format required by biwavelet dat &lt;- cbind(time_vec, dis_vec) dat &lt;- dat[1:1880,] # Compute wavelet transform wav &lt;- biwavelet::wt(dat) # Plot wavelet power spectrum biwavelet::plot.biwavelet(wav, plot.phase = FALSE, type = &quot;power.norm&quot;) We’ll explore wavelets more in the next chapter. "],["takeaways.html", "5.5 Takeaways", " 5.5 Takeaways There are many methods for looking at the spectra of time series. For unevenly sampled series, we are more limited in methods, and interpolation may cause artifacts in our analyses. "],["wavelet-transform-in-r.html", "6 Wavelet Transform in R", " 6 Wavelet Transform in R Wavelet analysis is one of the most misused tools in climate timeseries analysis, partially due to the popularity of this webpage, which used to feature an interactive plotter. Wavelets are a very broad topic in applied mathematics, which we won’t attempt to summarize here. In the paleoclimate context, the term nearly always refers to the Continuous Wavelet Transform, and 99% of the time this uses the Morlet wavelet, which is a generalization of Fourier bases to endow them with localization in time and scale. Accordingly, this demo focuses on how to obtain wavelet spectra from the iconic EPICA Dome C record by Jouzel et al. (2007), and how to establish its coherency with a record of atmospheric CO2. 6.0.1 Packages dplyr, ggplot2, tidyr, biwavelet Let’s load these: 6.0.2 Data description ice phase deuterium data (\\(\\delta\\)D) from Jouzel et al. (2007). \\(\\delta\\)D is a proxy for the temperature of snow formation above the site, taken to be representative of Antartic temperature over such timescales. gas phase CO2 measurements from Luthi et al (2008), corrected by Bereiter et al. (2015). "],["loading-and-visualizing-the-deltad-series.html", "6.1 Loading and visualizing the \\(\\delta\\)D series", " 6.1 Loading and visualizing the \\(\\delta\\)D series We’ll also start by formatting Age in kyr. dDdf &lt;- read.csv(&quot;https://github.com/LinkedEarth/Pyleoclim_util/raw/master/pyleoclim/data/edc3deuttemp2007.csv&quot;) dDdf &lt;- dDdf %&gt;% mutate(AgeKy = Age/1000) %&gt;% select(AgeKy, Deuterium, Temperature) head(dDdf) ## AgeKy Deuterium Temperature ## 1 0.03837379 -390.9 0.88 ## 2 0.04681203 -385.1 1.84 ## 3 0.05505624 -377.8 3.04 ## 4 0.06441511 -394.1 0.35 ## 5 0.07315077 -398.7 -0.42 ## 6 0.08193244 -395.9 0.05 Let’s see a simple plot ggplot(data=dDdf, mapping = aes(x=AgeKy, y=Deuterium)) + geom_line() + ylab(&quot;\\u03B4D (\\u2030)&quot;) + xlab(&quot;Age_(ky_BP)&quot;) + ggtitle(&quot;EPICA Dome C Deuterium&quot;) + theme_light() "],["sampling-interval.html", "6.2 Sampling interval", " 6.2 Sampling interval First, take a look at the distribution of time increments. We’ll use the base function diff() to extract the sampling intervals. deltaT &lt;- data.frame(&quot;ID&quot; = 1:(length(dDdf$AgeKy)-1), &quot;interval&quot; = diff(dDdf$AgeKy)) ggplot(data = deltaT, mapping = aes(x=interval)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The data are not evenly spaced, which is a challenge because most timeseries analysis methods (particularly Fourier and wavelet analysis) make the implicit assumption that data are evenly spaced, so applying those tools to unevenly-spaced data will result in methods not behaving as they are supposed to. We therefore interpolate the data at 500y resolution to enable the much faster CWT algorithm of Torrence &amp; Compo (1998) to run. We also standardize because the original data range is rather large, which could cause numerical issues. "],["wavelet-analysis.html", "6.3 Wavelet Analysis", " 6.3 Wavelet Analysis Lastly, we play with wavelet analysis, which may be used to “unfold” a spectrum and look at its evolution over time. There are several ways to access that functionality in Pyleoclim, but here we use summary_plot, which stacks together the timeseries itself, its scalogram (a plot of the magnitude of the wavelet power), and the power spectral density (PSD) obtained from summing the wavelet coefficients over time. Age.interp &lt;- seq(ceiling(min(dDdf$AgeKy)), floor(max(dDdf$AgeKy)), by=0.5) df2 &lt;- data.frame(&quot;AgeKy&quot; = Age.interp, &quot;Deuterium&quot; = rep(NA, length(Age.interp))) dDdf2 &lt;- merge.data.frame(dDdf, df2, all = T) dDdf3 &lt;- dDdf2 %&gt;% mutate(Temp.interp = approx(AgeKy, Temperature, AgeKy)$y) %&gt;% mutate(D.interp = approx(AgeKy, Deuterium, AgeKy)$y) %&gt;% dplyr::filter(AgeKy %in% Age.interp) %&gt;% select(AgeKy, D.interp, Temp.interp) %&gt;% rename(Age = AgeKy) head(dDdf3) ## Age D.interp Temp.interp ## 1 1.0 -398.6369 -0.42737296 ## 2 1.5 -398.3863 -0.40229635 ## 3 2.0 -401.5101 -0.93168060 ## 4 2.5 -394.8719 0.14813821 ## 5 3.0 -395.7483 -0.01805415 ## 6 3.5 -400.3498 -0.80095654 dDdf4 &lt;- dDdf3 %&gt;% select(-Temp.interp) dD_wave &lt;- biwavelet::wt(dDdf4, do.sig = F) par(oma = c(0, 0, 0, 1), mar = c(5, 4, 4, 5) + 0.1) plot(dD_wave, plot.cb=TRUE, plot.phase=FALSE) The scalogram reveals how spectral power (technically, wavelet power) changes over time. But which aspects of this are significant? 6.3.1 Null hypothesis There are ways to test this using an AR(1) benchmark. Let’s use the conditional-sum-of-squares method to fit the AR1 model. (see ?arima for details) dD_wave &lt;- biwavelet::wt(dDdf4, do.sig = TRUE, arima.method = &quot;CSS&quot;) par(oma = c(0, 0, 0, 1), mar = c(5, 4, 4, 5) + 0.1) plot(dD_wave, plot.cb=TRUE, plot.phase=FALSE) The solid lines delineate regions of the scalogram that are significant against an AR(1) benchmark, thus encircling “islands” of notable power. We see that the 100kyr periodicity is particularly pronounced around 300-600 kyr BP, while the 40 and 20kyr periodicities are more pronounced in the later portions (since 400 ky BP). This may be because of the compaction of the ice at depth, which you wouldn’t know from analyzing just this dataset. Paleoclimate timeseries must always be interpreted with those possible artifacts in mind. There are a lot of significant islands at short (&lt;5ky) scales as well, but it is unclear whether this is reflective of large-scale climate variability. "],["temperature-vs-co2.html", "6.4 Temperature vs CO2", " 6.4 Temperature vs CO2 Now let us load the CO2 composite from this and other neighboring sites around Antarctica: co2df &lt;- read.table(&#39;ftp://ftp.ncdc.noaa.gov/pub/data/paleo/icecore/antarctica/antarctica2015co2composite.txt&#39;, skip = 137, sep = &quot;\\t&quot;, header = T) head(co2df) ## age_gas_calBP co2_ppm co2_1s_ppm ## 1 -51.03 368.02 0.06 ## 2 -48.00 361.78 0.37 ## 3 -46.28 359.65 0.10 ## 4 -44.41 357.11 0.16 ## 5 -43.08 353.95 0.04 ## 6 -42.31 353.72 0.22 We’ll again scale the Age column to kyr, and we’ll rename the verbose columns co2df &lt;- co2df %&gt;% mutate(Age = age_gas_calBP/1000) %&gt;% rename(CO2 = co2_ppm) %&gt;% select(Age, CO2) head(co2df) ## Age CO2 ## 1 -0.05103 368.02 ## 2 -0.04800 361.78 ## 3 -0.04628 359.65 ## 4 -0.04441 357.11 ## 5 -0.04308 353.95 ## 6 -0.04231 353.72 Let’s have a look ggplot(data = co2df, mapping=aes(x=Age, y=CO2)) + geom_line() + labs(title = &quot;EPICA Dome C CO2&quot;, x=&quot;Age (ky BP)&quot;, y=expression(paste(CO^2, &quot; (ppm)&quot;, sep=&quot;&quot;))) + theme_light() "],["bringing-the-datasets-together.html", "6.5 Bringing the datasets together", " 6.5 Bringing the datasets together We see very similar Ice Ages as in the deuterium data and of course a precipitous rise since the Industrial Revolution. To plot the two series side by side, we’ll merge them into one data frame dDdfnew &lt;- dDdf %&gt;% rename(Age = AgeKy) %&gt;% select(Age, Deuterium) co2_dD &lt;- merge.data.frame(co2df, dDdfnew, all = T) co2_dD_long &lt;- gather(co2_dD, variable, value, -Age) co2_dD_long &lt;- na.omit(co2_dD_long) ggplot(data=co2_dD_long, mapping = aes(x=Age, y=value, color=variable)) + geom_line() + theme_light() By default, the ggplot assumes commensurate units, which is not really the case here. Fear not, we can just standardize the series: co2_dD_scaled &lt;- co2_dD %&gt;% mutate(across(-Age, ~ as.numeric(scale(.x)), .names = &quot;{.col}_z&quot;)) %&gt;% select(Age, CO2_z, Deuterium_z) %&gt;% rename(CO2 = CO2_z) %&gt;% rename(Deuterium = Deuterium_z) %&gt;% gather(key=variable, value = value, -Age) %&gt;% na.omit() ggplot(data=co2_dD_scaled, mapping = aes(x=Age, y=value, color=variable)) + geom_line() + labs(title = &quot;EPICA Dome C CO2 and Deuterium&quot;, x=&quot;Age (ky BP)&quot;, y=&quot;z-score&quot;) + theme_light() We seek to understand potential lead/lag relationships between those two series. Before that, a brief primer: the temperature proxy \\(\\delta\\)D is measured on the ice, whereas CO2 is measured in the air trapped in the ice. Because bubbles close only progressively as the firn gets compacted, the air can be younger than the surrouding ice by several hundred years. The ice core community has worked diligently on this for decades and have made very clever adjustments to correct for this effect, but that is to be kept in mind when comparing those two data streams. 6.5.1 Standardization With that in mind, let us interpolate this new record and merge the two. CO2.Age.interp &lt;- seq(ceiling(min(co2df$Age)), floor(max(co2df$Age)), by=0.5) CO2.interp.df &lt;- data.frame(&quot;Age&quot; = CO2.Age.interp, &quot;CO2&quot; = rep(NA, length(CO2.Age.interp))) co2df2 &lt;- merge.data.frame(co2df, CO2.interp.df, all = T) head(co2df2[is.na(co2df2$CO2),]) ## Age CO2 ## 62 0.0 NA ## 201 0.5 NA ## 225 1.0 NA ## 241 1.5 NA ## 259 2.0 NA ## 264 2.5 NA co2df3 &lt;- co2df2 %&gt;% mutate(CO2.interp = approx(Age, CO2, Age)$y) %&gt;% dplyr::filter(Age %in% CO2.Age.interp) %&gt;% select(Age, CO2.interp) interp.CO2.dD &lt;- merge(co2df3, dDdf4, all=T) head(interp.CO2.dD) ## Age CO2.interp D.interp ## 1 0.0 312.7155 NA ## 2 0.5 282.1503 NA ## 3 1.0 277.8846 -398.6369 ## 4 1.5 279.2627 -398.3863 ## 5 2.0 277.9966 -401.5101 ## 6 2.5 277.8299 -394.8719 6.5.2 Coherency Now we can apply wavelet transform coherency to identify phase relationships between the two series at various scales: # Wavelet coherence; nrands should be large (&gt;= 1000), using 100 for speed wtc1 &lt;- wtc(d1=co2df3[1:(dim(co2df3)[1]-10),], d2=dDdf4, nrands=100, quiet = TRUE) # Plot wavelet coherence and phase difference (arrows) # Make room to the right for the color bar par(oma=c(0, 0, 0, 1), mar=c(5, 4, 4, 5) + 0.1) plot(wtc1, plot.cb=TRUE,plot.phase = TRUE) This plot shows two things the wavelet transform coherency (WTC), which may be thought of as a (squared) correlation coefficient in time-scale space: 0 if the series do not covary, 1 if they vary in perfect unison at that time and scale. the phase angle between the two series, using a trigonometric convention (right = 0, top = 90 degrees, left = 180 degrees, bottom = -90 or + 270 degrees). This means that on orbital timescales, the two series are essentially in phase ; there is no consistent lead or lag between the two series. This is remarkable given the dating challenges mentioned earlier, and is widely interpreted to mean that on such timescales, atmospheric CO2 is a part of a positive feedback loop amplifying orbitally-triggered changes in temperature. However, the variance of this angle is fairly high, and by this test it does not register as a very consistent signal. Lastly, note that in the anthropogenic era, atmospheric CO2 is of course a forcing (the largest climate forcing, currently), acting on much, much faster timescales for which the climate system is still catching up. You would not know it from this analysis, but it’s important to state out loud, given that climate deniers have an annoying tendency to cherry-pick the paleoclimate record in support of their feeble arguments. 6.5.3 Common power Another consideration is that coherency is like the correlation coefficient in wavelet space: it tells you how similar two series are for a given time and scale, yet it says nothing about what fraction of the total variability is shared. This is better measured by the cross-wavelet transform (XWT), which highlights areas of high common power. Both of those, along with the original series, can be visualized with one swift function call: xwt1 &lt;- xwt(d1=co2df3[1:(dim(co2df3)[1]-10),], d2=dDdf4) # Plot cross-wavelet and phase difference (arrows) # Make room to the right for the color bar par(oma=c(0, 0, 0, 1), mar=c(5, 4, 4, 5) + 0.1) plot(xwt1, plot.cb=TRUE,plot.phase = TRUE) Here we see that the orbital bands are the only ones to show up consistently throughout the record, but precession and obliquity fade before 200 ky BP in their common power, and the XWT band near 120 ka drifts to 80ka back in time. This means that, out of the large swathes of WTC one would consider “coherent”, only those areas highlighted in XWT in green-yellow are likely important. That’s where we stop the banter, though: when you are getting down to that level, you had better be a glaciologist or talk to one about what is going on in those datasets. "],["takeways-2.html", "6.6 Takeways", " 6.6 Takeways evenly-spaced data are handled with the CWT algorithm of Torrence &amp; Compo (1998), which is very fast; unevenly-spaced data are handled through the WWZ algorithm of Foster (1996), which is slow. The temptation to interpolate is strong, but it has serious downsides (INTERPOLATION = MAKING UP DATA!). WWZ should be preferred, though CWT+interpolation can be used at exploratory stages. Wavelet Transform Coherency may be thought of as a (squared) correlation coefficient in time-scale space: 0 if the series do not covary, 1 if they vary in perfect unison at that time and scale. Properly interpreting the results of wavelet and wavelet coherence analyses requires knowing something about the methods. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
